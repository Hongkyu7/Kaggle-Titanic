{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv('train.csv', na_values=['', ' ', 'na', 'nan'])\n",
    "df_test_raw = pd.read_csv('test.csv', na_values=['', ' ', 'na', 'nan'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": "PassengerId      0\nPclass           0\nName             0\nSex              0\nAge             86\nSibSp            0\nParch            0\nTicket           0\nFare             1\nCabin          327\nEmbarked         0\ndtype: int64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train_raw.dtypes)\n",
    "\n",
    "##### 결측치 개수 확인 ######\n",
    "df_train_raw.isna().sum()\n",
    "# 891개 Data\n",
    "# 결측치 Age: 177개, Cabin: 687개, Embarked: 2개\n",
    "df_test_raw.isna().sum()\n",
    "# 418개 Data\n",
    "# 결측치 Age: 86개, Fare: 1개 Cabin: 327개\n",
    "###########################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, feature_list, method='pandas', nan_drop_boolean='True', original_feature_drop_boolean='True'):\n",
    "\n",
    "    p_df = df.copy()\n",
    "\n",
    "    if method == 'sklearn': # Using scikit learn\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        one_hot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "\n",
    "        for feature in feature_list:\n",
    "            p_df[feature] = p_df[feature].astype(str)\n",
    "            element_name = list(set(p_df[feature]))\n",
    "            exist_nan = False\n",
    "            if 'nan' in element_name: exist_nan = True\n",
    "\n",
    "            label_encoded = label_encoder.fit_transform(p_df[feature])\n",
    "            one_hot_encoded = one_hot_encoder.fit_transform(p_df[feature].to_numpy().reshape(-1,1))\n",
    "\n",
    "            df_one_hot_encoded = pd.DataFrame(one_hot_encoded)\n",
    "            p_df = pd.concat([p_df,df_one_hot_encoded], axis=1)\n",
    "\n",
    "            dict_col_name = {}\n",
    "            for i in range(len(element_name)):\n",
    "                dict_col_name[i] = feature + '_' + p_df[p_df[i] == 1][feature].iloc[0]\n",
    "\n",
    "            p_df.rename(columns=dict_col_name, inplace=True)\n",
    "\n",
    "            if nan_drop_boolean and exist_nan: p_df.drop([feature + '_nan'], axis=1, inplace=True)\n",
    "            if original_feature_drop_boolean: p_df.drop([feature], axis=1, inplace=True)\n",
    "\n",
    "    else: # Default - pandas get_dummies\n",
    "        for feature in feature_list:\n",
    "            df_one_hot_encoded = pd.get_dummies(p_df[feature], dummy_na=not(nan_drop_boolean))\n",
    "            df_one_hot_encoded = df_one_hot_encoded.astype(float)\n",
    "\n",
    "            element_name = df_one_hot_encoded.columns.to_list()\n",
    "            element_name_mod = [feature + '_' + str(i) for i in element_name]\n",
    "\n",
    "            dict_col_name = dict(zip(element_name, element_name_mod))\n",
    "\n",
    "            df_one_hot_encoded.rename(columns=dict_col_name, inplace=True)\n",
    "\n",
    "            p_df = pd.concat([p_df, df_one_hot_encoded], axis=1)\n",
    "\n",
    "            if original_feature_drop_boolean: p_df.drop([feature], axis=1, inplace=True)\n",
    "\n",
    "    return p_df\n",
    "\n",
    "\n",
    "def cabin_only_letter(df, feature_return, original_feature_drop_boolean='True'):\n",
    "    p_df = df.copy()\n",
    "    p_df['Cabin_dtype'] = list(map(lambda x: type(x).__name__, p_df['Cabin']))\n",
    "    p_df[feature_return] = p_df['Cabin'].copy()\n",
    "    p_df.loc[p_df['Cabin_dtype'] == 'str', 'Cabin_str'] = list(map(lambda x: x[0], p_df.loc[p_df['Cabin_dtype'] == 'str', 'Cabin_str']))\n",
    "    df[feature_return] = p_df[feature_return]\n",
    "    if original_feature_drop_boolean: df.drop(['Cabin'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def name_title(df, feature_return, original_feature_drop_boolean='True'):\n",
    "    p_df = df.copy()\n",
    "    list_title_origin = list(map(lambda x:x.split('.')[0].split(' ')[-1],list(df['Name'])))\n",
    "    list_title = []\n",
    "    for title in list_title_origin:\n",
    "        if title not in ['Mr', 'Miss', 'Mrs', 'Master']: title = np.nan\n",
    "        list_title.append(title)\n",
    "    p_df[feature_return] = list_title\n",
    "    if original_feature_drop_boolean: p_df.drop(['Name'], axis=1, inplace=True)\n",
    "    return p_df\n",
    "\n",
    "def age_to_int(df, feature_return, na_treat='mean', na_check_feature=True, original_feature_drop_boolean='True'):\n",
    "    p_df = df.copy()\n",
    "    p_df['Age_check'] = p_df['Age'].copy()\n",
    "    p_df.loc[p_df['Age'].notna(), 'Age_check'] = list(map(lambda x: (x-math.trunc(x))==0, p_df.loc[p_df['Age'].notna(), 'Age_check']))\n",
    "    p_df.loc[p_df['Age'].isna(), 'Age_check'] = False\n",
    "    p_df[feature_return] = p_df['Age'].copy()\n",
    "    if na_treat=='na': p_df.loc[~p_df['Age_check'], feature_return] = np.nan\n",
    "    elif na_treat=='max': p_df.loc[~p_df['Age_check'], feature_return] = p_df['Age'].max()\n",
    "    elif na_treat=='min': p_df.loc[~p_df['Age_check'], feature_return] = p_df['Age'].min()\n",
    "    else: p_df.loc[~p_df['Age_check'], feature_return] = p_df['Age'].mean()\n",
    "    if na_check_feature:\n",
    "        df[feature_return] = p_df['Age_check'].copy()\n",
    "    df[feature_return] = p_df[feature_return].copy()\n",
    "    if original_feature_drop_boolean: df.drop(['Age'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def scaling(df, feature_list, scaler_option='minmax', original_feature_drop_boolean='True'):\n",
    "    if scaler_option == 'std': scaler = preprocessing.StandardScaler()\n",
    "    else: scaler = preprocessing.MinMaxScaler()\n",
    "    p_df = df.copy()\n",
    "    for feature in feature_list:\n",
    "        p_df[feature + '_scaled'] = scaler.fit_transform(np.array(df[feature]).reshape(-1,1))\n",
    "    if original_feature_drop_boolean: p_df.drop(feature_list, axis=1, inplace=True)\n",
    "    return p_df\n",
    "\n",
    "\n",
    "def pre_train_test_merge(train, test, target, split_key):\n",
    "    list_feature = train.columns\n",
    "    df_train = train.copy()\n",
    "    df_test = test.copy()\n",
    "    df_test[target] = split_key\n",
    "    df_concat = pd.concat([df_train, df_test], axis=0, sort=True, ignore_index=True)\n",
    "    df_concat = df_concat[list_feature]\n",
    "    return df_concat\n",
    "\n",
    "\n",
    "def pre_train_test_split(merged_data, target, split_key, target_type):\n",
    "    df_concat = merged_data.copy()\n",
    "    df_train = df_concat[df_concat[target] != split_key].copy()\n",
    "    df_test = df_concat[df_concat[target] == split_key].copy()\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop=True)\n",
    "    df_test.drop([target], axis=1, inplace=True)\n",
    "    df_train[target] = df_train[target].astype(target_type)\n",
    "    return df_train, df_test\n",
    "\n",
    "# df_merged = pre_train_test_merge(df_train_raw, df_test_raw, \"Survived\", \"test_data\")\n",
    "# df_train, df_test = pre_train_test_split(df_merged, \"Survived\", \"test_data\", int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df_merged = pre_train_test_merge(df_train_raw, df_test_raw, \"Survived\", \"test_data\")\n",
    "\n",
    "df_merged = cabin_only_letter(df_merged, 'Cabin_str')\n",
    "df_merged = name_title(df_merged, 'Name_title')\n",
    "df_merged = age_to_int(df_merged, 'Age_int', na_treat='mean')\n",
    "df_merged = scaling(df_merged, ['Age_int', 'Fare'])\n",
    "df_merged = one_hot_encoding(df_merged, ['Embarked', 'Cabin_str', 'Name_title', 'Sex'], method='pandas', nan_drop_boolean=True)\n",
    "df_merged = df_merged.drop(['PassengerId', 'Ticket'], axis=1)\n",
    "\n",
    "df_train, df_test = pre_train_test_split(df_merged, \"Survived\", \"test_data\", int)\n",
    "\n",
    "df_train.to_csv('train_hongkyu.csv', index=False)\n",
    "df_test.to_csv('test_hongkyu.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "machine_learning",
   "language": "python",
   "display_name": "python3_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}